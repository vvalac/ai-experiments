{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  langchain langchain-experimental langchain-openai neo4j wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate environment and set example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "DIFFBOT_API_KEY = os.environ['DIFFBOT_API_KEY']\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFFBOT_NLP = DiffbotGraphTransformer(diffbot_api_key=DIFFBOT_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "query = \"Elon Musk\"\n",
    "raw_documents = WikipediaLoader(query=query).load()\n",
    "graph_documents = DIFFBOT_NLP.convert_to_graph_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe raw doc retrieval from wiki\n",
    "print(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a docker container to explore graph via GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a docker going via CLI\n",
    "# docker run \\\n",
    "#    --name neo4j \\\n",
    "#    -p 7474:7474 -p 7687:7687 \\\n",
    "#    -d \\\n",
    "#    -e NEO4J_AUTH=neo4j/pleaseletmein \\\n",
    "#    -e NEO4J_PLUGINS=\\[\\\"apoc\\\"\\]  \\\n",
    "#    neo4j:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "# if running docker in WSL, command wsl hostname -I to get WSL IP\n",
    "# neo4j gui port is 7474\n",
    "url = \"bolt://172.29.153.143:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"pleaseletmein\"\n",
    "\n",
    "graph = Neo4jGraph(url=url, username=username, password=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GraphDocuments into the docker knowledge graph then refresh graph schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a graph query manually to verify connectivity before burning OpenAI calls\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "MATCH p=()-[r:CHIEF_EXECUTIVE_OFFICER]->() RETURN p LIMIT 25\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log into Neo4J by navigating to http://\\{docker ip}/7474 and logging in with credentials set above. To view the graph, you can run a simple query of `MATCH (n) RETURN n LIMIT 25`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-0125-preview\"),\n",
    "    qa_llm=ChatOpenAI(temperature=.3, model_name=\"gpt-3.5-turbo\"),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Does Elon Musk have kids?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saas-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
